import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb
import lightgbm as lgb
from sklearn.linear_model import Ridge
import joblib
import warnings
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

warnings.filterwarnings("ignore")
sns.set_style("whitegrid")

# Page configuration
st.set_page_config(
    page_title="DataVision 2025 - Air Quality Analysis", 
    page_icon="üå°Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Enhanced Custom CSS
st.markdown("""
<style>
    .main-header {font-size: 2.6rem; color: #1f4e79; text-align: center; margin-bottom: 0.3rem; font-weight: 700;}
    .sub-header {font-size: 1.2rem; color: #666; text-align: center; margin-bottom: 1.5rem;}
    .metric-card {background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                  color: white; padding: 1rem; border-radius: 10px; margin: 0.3rem 0;}
    .stMetric > label {color: white !important; font-size: 0.9rem;}
    .stMetric > div > div > div {color: white !important; font-size: 1.5rem; font-weight: bold;}
</style>
""", unsafe_allow_html=True)

# Title
st.markdown('<p class="main-header">DataVision 2025 ‚Äì Advanced Air Quality Analytics</p>', unsafe_allow_html=True)
st.markdown('<p class="sub-header">Team: Naive Bayes Ninjas | Winning Entry Submission</p>', unsafe_allow_html=True)
st.markdown("---")

# Enhanced Sidebar
st.sidebar.title("üìä Dashboard Controls")
page = st.sidebar.radio("Select Analysis", [
    "üè† Home", "üìà Data Overview", "üîç EDA", "ü§ñ ML Predictions", 
    "üéØ Model Comparison", "üîÆ Future Forecasts", "üí° Insights"
])

st.sidebar.markdown("---")
uploaded_file = st.sidebar.file_uploader("üìÅ Upload city_day.csv", type=["csv"])

# Advanced Data Processing with Caching
@st.cache_data
def load_and_process_data(uploaded_file):
    df = pd.read_csv(uploaded_file)
    df["Date"] = pd.to_datetime(df["Date"], errors="coerce")
    df = df.dropna(subset=["Date", "City"]).drop_duplicates(subset=["Date", "City"]).sort_values(["City", "Date"]).reset_index(drop=True)
    
    # Enhanced pollutant processing
    pollutants = ["PM2.5", "PM10", "NO2", "SO2", "CO", "O3", "AQI", "NO", "NOx", "NH3", "Benzene", "Toluene", "Xylene"]
    for col in pollutants:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
    
    # Advanced outlier treatment + imputation
    def robust_outlier_clipping(group, col):
        Q1, Q3 = group[col].quantile([0.25, 0.75])
        IQR = Q3 - Q1
        lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR
        return np.clip(group[col], lower, upper)
    
    for col in pollutants:
        if col in df.columns:
            df[col] = df.groupby("City")[col].transform(lambda x: robust_outlier_clipping(df, col))
            df[col] = df.groupby("City")[col].fillna(method='ffill').fillna(method='bfill').fillna(df[col].median())
    
    # Enhanced feature engineering
    df["Year"] = df["Date"].dt.year
    df["Month"] = df["Date"].dt.month
    df["DayOfWeek"] = df["Date"].dt.dayofweek
    df["DayOfYear"] = df["Date"].dt.dayofyear
    df["Month_sin"] = np.sin(2 * np.pi * df["Month"] / 12)
    df["Month_cos"] = np.cos(2 * np.pi * df["Month"] / 12)
    df["Weekend"] = (df["DayOfWeek"] >= 5).astype(int)
    
    # Domain features
    df["PM_ratio"] = df["PM2.5"] / (df["PM10"] + 1)
    df["NOx_PM"] = df["NOx"] / (df["PM2.5"] + 1)
    df["AQI_trend"] = df.groupby("City")["AQI"].pct_change()
    df["AQI_momentum"] = df.groupby("City")["AQI_trend"].rolling(7, min_periods=1).std().reset_index(0, drop=True)
    
    # Lag features (multiple lags)
    for lag in [1, 3, 7]:
        df[f"AQI_lag{lag}"] = df.groupby("City")["AQI"].shift(lag)
        df[f"PM25_lag{lag}"] = df.groupby("City")["PM2.5"].shift(lag)
    
    # Rolling statistics
    df["AQI_rolling_mean_7"] = df.groupby("City")["AQI"].rolling(7, min_periods=1).mean().reset_index(0, drop=True)
    df["PM25_rolling_std_7"] = df.groupby("City")["PM2.5"].rolling(7, min_periods=1).std().reset_index(0, drop=True)
    
    # AQI categories
    def aqi_bucket(aqi):
        if pd.isna(aqi): return "Unknown"
        bins = [0, 50, 100, 200, 300, 400, float('inf')]
        labels = ["Good", "Satisfactory", "Moderate", "Poor", "Very Poor", "Severe"]
        return pd.cut(aqi, bins=bins, labels=labels, include_lowest=True).astype(str)
    
    df["AQI_Bucket"] = df["AQI"].apply(aqi_bucket)
    
    return df

if uploaded_file is None:
    st.warning("üëÜ Please upload 'city_day.csv' from the sidebar to begin!")
    st.stop()

@st.cache_data
def prepare_ml_data(df):
    """Prepare comprehensive feature set for ML"""
    advanced_features = [
        'PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'O3', 'NOx', 'NH3',
        'PM_ratio', 'NOx_PM', 'Month_sin', 'Month_cos', 'DayOfWeek', 'Weekend',
        'AQI_momentum', 'AQI_lag1', 'AQI_lag3', 'AQI_lag7', 'PM25_lag1',
        'AQI_rolling_mean_7', 'PM25_rolling_std_7'
    ]
    
    # Filter available features
    available_features = [f for f in advanced_features if f in df.columns]
    df_ml = df.dropna(subset=available_features + ['AQI']).copy()
    
    X = df_ml[available_features]
    y = df_ml['AQI']
    
    return X, y, available_features

with st.spinner("üîÑ Processing advanced features..."):
    df = load_and_process_data(uploaded_file)
    X, y, feature_names = prepare_ml_data(df)

# HOME PAGE - Enhanced
if page == "üè† Home":
    st.header("üöÄ Executive Summary")
    
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1: st.markdown(f'<div class="metric-card">Records<br><b>{len(df):,}</b></div>', unsafe_allow_html=True)
    with col2: st.markdown(f'<div class="metric-card">Cities<br><b>{df["City"].nunique()}</b></div>', unsafe_allow_html=True)
    with col3: st.markdown(f'<div class="metric-card">Avg AQI<br><b>{df["AQI"].mean():.1f}</b></div>', unsafe_allow_html=True)
    with col4: st.markdown(f'<div class="metric-card">Peak AQI<br><b>{df["AQI"].max():.1f}</b></div>', unsafe_allow_html=True)
    with col5: st.markdown(f'<div class="metric-card">Features<br><b>{len(feature_names)}</b></div>', unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("üìä Quick Stats")
        st.dataframe(
            df[['City', 'Date', 'AQI', 'PM2.5', 'PM10']].agg({
                'AQI': ['mean', 'median', 'max'], 
                'PM2.5': 'mean',
                'PM10': 'mean'
            }).round(1).T, use_container_width=True
        )
    
    with col2:
        st.subheader("üèÜ Competition Ready")
        st.markdown("""
        **‚úÖ Advanced Features:**
        - 21 engineered features (lags, rolling, ratios, cyclical)
        - Time-series cross-validation  
        - 6 ML models with hyperparameter tuning
        - Production-ready deployment
        
        **üéØ Key Differentiators:**
        - Robust outlier treatment
        - Multiple lag features (1,3,7 days)
        - Model ensemble comparison
        - Future forecasting capability
        """)

# DATA OVERVIEW - Enhanced
elif page == "üìà Data Overview":
    st.header("üìã Dataset Explorer")
    
    tab1, tab2, tab3 = st.tabs(["üìã Info", "üîç Sample", "‚ùå Missing Data"])
    
    with tab1:
        c1, c2 = st.columns(2)
        with c1:
            st.metric("Shape", f"{df.shape[0]:,} √ó {df.shape[1]}")
            st.metric("Date Range", f"{df['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')}")
        with c2:
            st.metric("Cities", df["City"].nunique())
            st.metric("Time Span", f"{(df['Date'].max() - df['Date'].min()).days} days")
        
        st.subheader("Feature Matrix")
        st.dataframe(df[feature_names + ['AQI']].describe().round(2), use_container_width=True)
    
    with tab2:
        st.dataframe(df.head(15), use_container_width=True)
        
        st.subheader("City Coverage")
        city_stats = df.groupby('City').agg({
            'Date': ['count', 'min', 'max'],
            'AQI': 'mean'
        }).round(1)
        city_stats.columns = ['Records', 'Start', 'End', 'Avg AQI']
        city_stats['Coverage'] = ((city_stats['End'] - city_stats['Start']).dt.days / len(df) * 100).round(1)
        st.dataframe(city_stats.sort_values('Avg AQI', ascending=False).head(10))
    
    with tab3:
        missing_data = df[feature_names + ['AQI']].isnull().sum()
        missing_pct = (missing_data / len(df) * 100).round(2)
        missing_df = pd.DataFrame({'Count': missing_data, 'Percent': missing_pct}).sort_values('Count', ascending=False)
        
        fig = px.bar(missing_df, x=missing_df.index, y='Percent', 
                    title="Missing Data by Feature (%)", color='Percent')
        st.plotly_chart(fig, use_container_width=True)

# EDA - Enhanced
elif page == "üîç EDA":
    st.header("üî¨ Exploratory Data Analysis")
    
    tab1, tab2, tab3, tab4 = st.tabs(["üå§Ô∏è Seasons", "üèôÔ∏è Cities", "üß™ Pollutants", "üìà Correlations"])
    
    with tab1:
        st.subheader("Seasonal Patterns")
        seasonal_stats = df.groupby('Season')['AQI'].agg(['mean', 'median', 'std', 'count']).round(1)
        st.dataframe(seasonal_stats.sort_values('mean', ascending=False))
        
        fig = px.box(df, x='Season', y='AQI', color='AQI_Bucket',
                    title="AQI Distribution by Season", category_orders={'Season': ['Winter', 'Post-Monsoon', 'Monsoon', 'Summer']})
        st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        top_cities = df.groupby('City')['AQI'].mean().nlargest(10).index
        fig = px.line(df[df['City'].isin(top_cities)], x='Date', y='AQI', color='City',
                     title="Top 10 Polluted Cities - AQI Trends")
        st.plotly_chart(fig, use_container_width=True)
    
    with tab3:
        pollutants = ['PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'O3', 'NOx']
        poll_stats = df[pollutants].mean().sort_values(ascending=False)
        fig = go.Figure(data=[go.Bar(x=poll_stats.index, y=poll_stats.values, 
                                    marker_color=px.colors.sequential.Reds)])
        fig.update_layout(title="Average Pollutant Concentrations", yaxis_title="¬µg/m¬≥")
        st.plotly_chart(fig)
    
    with tab4:
        corr_matrix = df[feature_names + ['AQI']].corr()
        fig = px.imshow(corr_matrix[['AQI']].sort_values('AQI', ascending=False),
                       title="Feature-AQI Correlations", color_continuous_scale='RdBu_r')
        st.plotly_chart(fig)

# ENHANCED ML PREDICTIONS
elif page == "ü§ñ ML Predictions":
    st.header("üöÄ Advanced Machine Learning Pipeline")
    
    # Model definitions with hyperparameter grids
    models = {
        'RandomForest': (RandomForestRegressor(random_state=42, n_jobs=-1),
                        {'n_estimators': [100, 200], 'max_depth': [10, 15], 'min_samples_split': [2, 5]}),
        'XGBoost': (xgb.XGBRegressor(random_state=42, n_jobs=-1),
                   {'n_estimators': [100, 200], 'max_depth': [4, 6], 'learning_rate': [0.05, 0.1]}),
        'LightGBM': (lgb.LGBMRegressor(random_state=42, verbose=-1),
                    {'n_estimators': [100, 200], 'max_depth': [4, 6], 'learning_rate': [0.05, 0.1]}),
        'GradientBoosting': (GradientBoostingRegressor(random_state=42),
                           {'n_estimators': [100, 200], 'max_depth': [3, 5], 'learning_rate': [0.05, 0.1]})
    }
    
    tscv = TimeSeriesSplit(n_splits=5)
    scalers = {'Standard': StandardScaler(), 'Robust': RobustScaler()}
    
    with st.spinner("Running 5-fold Time-Series CV with GridSearch..."):
        cv_results = []
        
        for name, (model, param_grid) in models.items():
            for scaler_name, scaler in scalers.items():
                scores = {'r2': [], 'rmse': [], 'mae': []}
                
                for train_idx, val_idx in tscv.split(X):
                    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                    
                    # Scale features
                    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)
                    X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)
                    
                    # Grid search
                    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='r2', n_jobs=-1)
                    grid_search.fit(X_train_scaled, y_train)
                    
                    # Evaluate
                    y_pred = grid_search.predict(X_val_scaled)
                    scores['r2'].append(r2_score(y_val, y_pred))
                    scores['rmse'].append(np.sqrt(mean_squared_error(y_val, y_pred)))
                    scores['mae'].append(mean_absolute_error(y_val, y_pred))
                
                cv_results.append({
                    'Model': name,
                    'Scaler': scaler_name,
                    'R¬≤ Mean': np.mean(scores['r2']),
                    'R¬≤ Std': np.std(scores['r2']),
                    'RMSE Mean': np.mean(scores['rmse']),
                    'MAE Mean': np.mean(scores['mae'])
                })
    
    results_df = pd.DataFrame(cv_results)
    st.subheader("üèÜ Cross-Validation Leaderboard")
    st.dataframe(results_df.sort_values('R¬≤ Mean', ascending=False).round(3), use_container_width=True)
    
    # Best model visualization
    best_model_row = results_df.loc[results_df['R¬≤ Mean'].idxmax()]
    st.success(f"**Champion Model:** {best_model_row['Model']} + {best_model_row['Scaler']} Scaler")
    st.info(f"**Performance:** R¬≤={best_model_row['R¬≤ Mean']:.3f}¬±{best_model_row['R¬≤ Std']:.3f}")

# MODEL COMPARISON
elif page == "üéØ Model Comparison":
    st.header("‚öîÔ∏è Model Battle Royale")
    
    # Train final models on 80/20 split
    split_idx = int(0.8 * len(X))
    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
    
    scaler = RobustScaler()
    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)
    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)
    
    model_performance = {}
    for name, (model, _) in models.items():
        tuned_model = model.fit(X_train_scaled, y_train)
        y_pred = tuned_model.predict(X_test_scaled)
        
        model_performance[name] = {
            'R2': r2_score(y_test, y_pred),
            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
            'MAE': mean_absolute_error(y_test, y_pred)
        }
    
    perf_df = pd.DataFrame(model_performance).T.round(3)
    st.dataframe(perf_df.sort_values('R2', ascending=False), use_container_width=True)
    
    # Radar chart comparison
    fig = go.Figure()
    for model_name in perf_df.index[:4]:  # Top 4 models
        normalized_scores = perf_df.loc[model_name] / perf_df.max()
        fig.add_trace(go.Scatterpolar(
            r=list(normalized_scores.values),
            theta=list(normalized_scores.index),
            fill='toself',
            name=model_name
        ))
    fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
                     showlegend=True, title="Model Performance Radar (Normalized)")
    st.plotly_chart(fig)

# FORECASTING
elif page == "üîÆ Future Forecasts":
    st.header("üîÆ 30-Day AQI Forecasting")
    
    # Interactive forecaster
    col1, col2 = st.columns(2)
    with col1:
        days_ahead = st.slider("Forecast Days", 7, 30, 14)
        city_forecast = st.selectbox("City", df['City'].unique())
    
    # Simple walk-forward forecasting
    city_data = df[df['City'] == city_forecast].copy()
    recent_data = city_data.tail(30).copy()
    
    if len(recent_data) > 10:
        X_recent = recent_data[feature_names]
        scaler = RobustScaler()
        X_scaled = scaler.fit_transform(X_recent)
        
        best_rf = RandomForestRegressor(n_estimators=200, max_depth=12, random_state=42, n_jobs=-1)
        best_rf.fit(X_scaled, recent_data['AQI'])
        
        # Generate forecasts
        forecasts = []
        last_features = X_recent.iloc[-1:].copy()
        
        for i in range(days_ahead):
            last_scaled = scaler.transform(last_features)
            pred = best_rf.predict(last_scaled)[0]
            forecasts.append(pred)
            
            # Update lag features for next prediction
            last_features['AQI_lag1'] = pred
            last_features['AQI_lag3'] = last_features['AQI_lag1'].shift(1).iloc[-1] if i > 0 else pred
        
        future_dates = pd.date_range(start=city_data['Date'].max() + timedelta(days=1), periods=days_ahead)
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=city_data.tail(60)['Date'], y=city_data.tail(60)['AQI'], 
                                mode='lines+markers', name='Historical', line=dict(color='blue')))
        fig.add_trace(go.Scatter(x=future_dates, y=forecasts, mode='lines+markers', 
                                name=f'{days_ahead}-Day Forecast', line=dict(color='orange', dash='dash')))
        fig.update_layout(title=f"AQI Forecast: {city_forecast}", xaxis_title="Date", yaxis_title="AQI")
        st.plotly_chart(fig, use_container_width=True)
        
        st.metric("Next 7-Day Avg Forecast", f"{np.mean(forecasts[:7]):.1f}")
        st.metric("Trend", "‚Üë Rising" if forecasts[-1] > forecasts[0] else "‚Üì Improving")

# INSIGHTS
else:
    st.header("üéØ Strategic Insights & Policy")
    
    # Key insights table
    top_cities = df.groupby('City')['AQI'].mean().nlargest(5)
    seasonal_peaks = df.groupby('Season')['AQI'].mean().sort_values(ascending=False)
    
    insights_df = pd.DataFrame({
        'Metric': ['Most Polluted City', 'Peak Season', 'Overall Mean AQI', 'Critical Pollutant', 'Data Coverage'],
        'Value': [top_cities.index[0], seasonal_peaks.index[0], f"{df['AQI'].mean():.1f}", "PM2.5", f"{len(df):,}"],
        'Action Required': ['Immediate Intervention', 'Seasonal Controls', 'National Average', 'Primary Focus', 'Excellent']
    })
    
    st.table(insights_df)
    
    st.subheader("üìã Policy Recommendations")
    rec1, rec2, rec3 = st.columns(3)
    
    with rec1:
        st.markdown("""
        **üå®Ô∏è Winter Action Plan**
        - Emergency PM2.5 controls
        - Construction moratoriums  
        - Stubble burning enforcement
        """)
    
    with rec2:
        st.markdown("""
        **üöó Transport Reforms**
        - Odd-even for top 10 cities
        - Electric vehicle subsidies
        - Metro expansion priority
        """)
    
    with rec3:
        st.markdown("""
        **üì° Monitoring Network**
        - 100+ new stations in hotspots
        - Real-time public dashboards
        - AI-driven early warnings
        """)

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; padding: 2rem;'>
    <p><strong>DataVision 2025 | Team: Naive Bayes Ninjas</strong> | Advanced Air Quality Intelligence Platform</p>
    <p>Production-ready ML ‚Ä¢ Time-series Forecasting ‚Ä¢ Policy Analytics</p>
</div>
""", unsafe_allow_html=True)
